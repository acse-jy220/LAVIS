{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2b249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b872f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models.blip2_models.Qformer import BertLMHeadModel, BertModel, BertOnlyMLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c5b62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = torch.load(\"/mnt/d/QFormer/query_tokens.pth\", map_location=\"cpu\")\n",
    "image_embeds = torch.load(\"/mnt/d/QFormer/image_embeds.pth\", map_location=\"cpu\")\n",
    "image_atts = torch.load(\"/mnt/d/QFormer/image_atts.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec25889c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b756a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 257, 1408])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "029611f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 257])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_atts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e935f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers.configuration_utils import PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc822a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_config = PretrainedConfig()\n",
    "pretrained_config = pretrained_config.from_pretrained(\"/mnt/d/QFormer/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b98a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output_real = torch.load(\"/mnt/d/QFormer/query_output.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6e3d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"/mnt/d/QFormer/self_Qformer_state_dict_init.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df653326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params:  336\n",
      "bert.embeddings.position_ids torch.Size([1, 512])\n",
      "bert.embeddings.word_embeddings.weight torch.Size([30523, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.0.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.0.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.2.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.2.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.4.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.4.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.6.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.6.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.8.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.8.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.10.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.10.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output_query.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([30523])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.decoder.weight torch.Size([30523, 768])\n",
      "cls.predictions.decoder.bias torch.Size([30523])\n"
     ]
    }
   ],
   "source": [
    "print(\"total params: \", len(state_dict))\n",
    "for name, tensor in state_dict.items():\n",
    "    print(name, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaba84ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params:  334\n",
      "bert.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.0.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.0.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.2.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.2.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.4.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.4.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.6.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.6.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.8.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.8.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.10.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.10.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output_query.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.decoder.weight torch.Size([30522, 768])\n"
     ]
    }
   ],
   "source": [
    "qFormer = BertLMHeadModel(pretrained_config)\n",
    "print(\"total params: \", len(list(qFormer.named_parameters())))\n",
    "for name, tensor in qFormer.named_parameters():\n",
    "    print(name, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f9f0e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params:  333\n",
      "bert.embeddings.word_embeddings.weight torch.Size([30523, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.0.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.0.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.2.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.2.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.4.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.4.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.6.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.6.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.8.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.8.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.crossattention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.key.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.10.crossattention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.self.value.weight torch.Size([768, 1408])\n",
      "bert.encoder.layer.10.crossattention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.crossattention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output_query.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate_query.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate_query.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output_query.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output_query.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output_query.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output_query.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([30523])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "qFormer.resize_token_embeddings(30523)\n",
    "print(\"total params: \", len(list(qFormer.named_parameters())))\n",
    "for name, tensor in qFormer.named_parameters():\n",
    "    print(name, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f12ec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30523"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qFormer.get_input_embeddings().num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47a6060b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qFormer.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11bbd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in qFormer.named_parameters():\n",
    "#     if \"_query\" in name:\n",
    "#         key_orig = name.replace(\"_query\", \"\")\n",
    "#         param.data.copy_(state_dict[key_orig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e51b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in qFormer.named_parameters():\n",
    "    if \"_query\" in name:\n",
    "        key_orig = name.replace(\"_query\", \"\")\n",
    "        param.data = state_dict[key_orig].data\n",
    "        # param.data.copy_(state_dict[key_orig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c99e3fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21946fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output = qFormer.bert(\n",
    "           query_embeds=query_tokens,\n",
    "           encoder_hidden_states=image_embeds,\n",
    "           encoder_attention_mask=image_atts,\n",
    "           use_cache=True,\n",
    "           return_dict=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8d470ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs1 = query_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d89af091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9438e-01,  7.3882e-03,  6.3201e-01,  ...,  8.8565e-02,\n",
       "           8.0319e-02, -1.7347e-01],\n",
       "         [-1.8025e-01,  1.7617e-02,  5.4297e-01,  ...,  7.7662e-02,\n",
       "           9.2035e-02, -2.6754e-01],\n",
       "         [-4.8205e-02, -4.9403e-02,  6.0834e-01,  ..., -6.8633e-02,\n",
       "           2.9498e-02, -3.5586e-01],\n",
       "         ...,\n",
       "         [-1.0602e-01, -1.1701e-01,  6.5464e-01,  ..., -2.2739e-03,\n",
       "           1.6734e-02, -3.0597e-01],\n",
       "         [-1.9081e-01, -8.9147e-02,  6.2723e-01,  ..., -2.7298e-02,\n",
       "           2.5504e-02, -2.7357e-01],\n",
       "         [-1.6533e-01,  4.0572e-03,  6.0402e-01,  ...,  7.5947e-02,\n",
       "           2.8548e-02, -3.6183e-01]],\n",
       "\n",
       "        [[ 9.7987e-02,  3.8709e-01,  4.9894e-01,  ...,  2.6643e-01,\n",
       "          -7.2123e-02, -6.5843e-02],\n",
       "         [ 6.0674e-02,  4.1230e-01,  4.0371e-01,  ...,  2.2618e-01,\n",
       "          -3.8215e-02, -1.7246e-01],\n",
       "         [ 3.1859e-01,  2.8222e-01,  4.5629e-01,  ...,  9.2004e-05,\n",
       "          -1.2570e-01, -4.0215e-01],\n",
       "         ...,\n",
       "         [ 7.3456e-02,  3.0670e-01,  6.3764e-01,  ...,  2.4000e-01,\n",
       "          -1.5797e-01, -3.9908e-01],\n",
       "         [ 1.0431e-01,  2.7425e-01,  4.1037e-01,  ...,  1.0340e-01,\n",
       "          -1.1187e-01, -2.3513e-01],\n",
       "         [ 1.8008e-02,  1.5624e-01,  5.8489e-01,  ...,  4.0834e-01,\n",
       "          -2.6123e-01, -2.8856e-01]],\n",
       "\n",
       "        [[-2.7353e-01,  4.7054e-01,  6.2327e-01,  ...,  9.5498e-02,\n",
       "          -1.3359e-01, -2.3273e-01],\n",
       "         [-2.7437e-01,  4.9316e-01,  5.1340e-01,  ...,  9.0985e-02,\n",
       "          -9.0530e-02, -3.5168e-01],\n",
       "         [-1.3848e-01,  4.4923e-01,  5.9953e-01,  ..., -7.1305e-02,\n",
       "          -1.9550e-01, -4.2164e-01],\n",
       "         ...,\n",
       "         [-1.4649e-01,  4.3746e-01,  6.4441e-01,  ..., -1.2410e-02,\n",
       "          -1.8001e-01, -3.4867e-01],\n",
       "         [-2.7185e-01,  3.4743e-01,  6.3086e-01,  ..., -3.4060e-02,\n",
       "          -2.0988e-01, -3.7922e-01],\n",
       "         [-2.6464e-01,  2.7410e-01,  5.9776e-01,  ...,  9.5911e-02,\n",
       "          -2.2029e-01, -4.6690e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.6574e-01,  1.3097e-01,  7.1138e-01,  ...,  3.4543e-01,\n",
       "          -7.8140e-02, -3.0976e-01],\n",
       "         [ 2.0554e-01,  1.6294e-01,  5.9129e-01,  ...,  3.0827e-01,\n",
       "          -9.5740e-02, -4.3120e-01],\n",
       "         [ 4.8691e-01,  1.3157e-02,  6.3999e-01,  ...,  1.1508e-01,\n",
       "          -1.9167e-01, -5.6477e-01],\n",
       "         ...,\n",
       "         [ 2.4922e-01,  5.4923e-02,  8.3448e-01,  ...,  3.6923e-01,\n",
       "          -2.0284e-01, -4.8795e-01],\n",
       "         [ 2.0520e-01, -2.4130e-02,  6.9519e-01,  ...,  1.7021e-01,\n",
       "          -1.4396e-01, -4.4742e-01],\n",
       "         [ 2.6194e-01,  1.2931e-01,  5.7867e-01,  ...,  2.5024e-01,\n",
       "          -2.4666e-01, -6.3693e-01]],\n",
       "\n",
       "        [[-6.2137e-02,  2.6298e-01,  3.5715e-01,  ...,  2.5791e-01,\n",
       "          -9.2674e-02,  7.3986e-02],\n",
       "         [-6.2316e-02,  2.6839e-01,  2.7936e-01,  ...,  2.3862e-01,\n",
       "          -6.3336e-02, -1.1669e-02],\n",
       "         [ 6.6566e-02,  2.1817e-01,  2.9101e-01,  ...,  4.2039e-02,\n",
       "          -1.7329e-01, -1.9245e-01],\n",
       "         ...,\n",
       "         [-2.0672e-02,  2.2129e-01,  4.3092e-01,  ...,  1.9177e-01,\n",
       "          -1.9228e-01, -1.0152e-01],\n",
       "         [-7.3828e-02,  1.6249e-01,  2.9410e-01,  ...,  1.2917e-01,\n",
       "          -1.5245e-01, -5.4993e-02],\n",
       "         [-3.9821e-02,  1.0576e-01,  4.0466e-01,  ...,  2.6552e-01,\n",
       "          -2.7862e-01, -1.5822e-01]],\n",
       "\n",
       "        [[ 1.8595e-01,  2.3552e-01,  6.6564e-01,  ...,  1.7013e-01,\n",
       "           7.3492e-02,  6.5128e-02],\n",
       "         [ 1.4086e-01,  2.3324e-01,  6.0213e-01,  ...,  1.5824e-01,\n",
       "           9.6736e-02, -3.9777e-02],\n",
       "         [ 3.3027e-01,  1.0893e-01,  5.8652e-01,  ..., -3.9149e-02,\n",
       "           7.6474e-02, -1.0192e-01],\n",
       "         ...,\n",
       "         [ 1.9951e-01,  1.0799e-01,  6.8128e-01,  ...,  7.2795e-02,\n",
       "           6.1475e-02, -1.1327e-01],\n",
       "         [ 1.8324e-01,  1.4615e-01,  6.5116e-01,  ...,  2.4357e-02,\n",
       "           4.6248e-02, -4.0213e-02],\n",
       "         [ 9.7861e-02,  7.3243e-02,  6.2137e-01,  ...,  1.1051e-01,\n",
       "          -8.3327e-02, -1.6563e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output2 = qFormer.bert(\n",
    "           query_embeds=query_tokens,\n",
    "           encoder_hidden_states=image_embeds,\n",
    "           encoder_attention_mask=image_atts,\n",
    "           use_cache=True,\n",
    "           return_dict=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs2 = query_output2.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs2.equal(hs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8331b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs3 = query_output_real.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953afd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs3.equal(hs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917f53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "l = torch.arange(100).reshape(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be747bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.shape\n",
    "l.t().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ad8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops as P\n",
    "m = P.arange(100).reshape(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05598738",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense.weight.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops as P\n",
    "expand_x = P.BroadcastTo((1, pretrained_config.max_position_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3842740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import numpy as np\n",
    "l = P.arange(pretrained_config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caaa555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a56a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.arange(pretrained_config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = n.expand((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd8fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_x(l).shape == nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59533e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.common.initializer import initializer, One, Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = initializer(Zero(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c246de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f56e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = Parameter(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35942078",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------ split line ---------------------------------------------------------------\n",
    "\n",
    "## <\\br> Initialize mindspore implement of Q-Former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe7506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "pretrained_config = PretrainedConfig()\n",
    "pretrained_config = pretrained_config.from_pretrained(\"/mnt/d/QFormer/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a02fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "ms.set_context(mode=ms.GRAPH_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00d3bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/GeneYu/desktop/mindformers/mindformers/core/clip_grad.py:39: RuntimeWarning: divide by zero encountered in log\n",
      "  inf = Tensor(np.log(0.0), mstype.float32)\n"
     ]
    }
   ],
   "source": [
    "from mindformers.models.blip2 import QFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1a8d52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter config in `BertLMHeadModel(config)` should be an instance of class `BaseConfig`. To create a model from a pretrained model use `model = BertLMHeadModel.from_pretrained(PRETRAINED_MODEL_NAME)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qFormer2 \u001b[38;5;241m=\u001b[39m \u001b[43mQFormer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBertLMHeadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/GeneYu/desktop/mindformers/mindformers/models/blip2/QFormer.py:1559\u001b[0m, in \u001b[0;36mBertLMHeadModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: QFormerConfig):\n\u001b[0;32m-> 1559\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBertLMHeadModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, QFormerConfig):\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1562\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter config in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(config)` should be an instance of class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1563\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`QFormerConfig`. To create a model from a pretrained model use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1564\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1565\u001b[0m         )\n",
      "File \u001b[0;32m/mnt/c/Users/GeneYu/desktop/mindformers/mindformers/models/blip2/QFormer.py:886\u001b[0m, in \u001b[0;36mBertPreTrainedModel.__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28msuper\u001b[39m(BertPreTrainedModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, BertConfig):\n\u001b[0;32m--> 886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter config in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(config)` should be an instance of class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`BaseConfig`. To create a model from a pretrained model use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     )\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# Save config and origin of the pretrained weights if given in model\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter config in `BertLMHeadModel(config)` should be an instance of class `BaseConfig`. To create a model from a pretrained model use `model = BertLMHeadModel.from_pretrained(PRETRAINED_MODEL_NAME)`"
     ]
    }
   ],
   "source": [
    "qFormer2 = QFormer.BertLMHeadModel(pretrained_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18487c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mindspore_params(network):\n",
    "    ms_params = {}\n",
    "    param_cnt = 0\n",
    "    for param in network.get_parameters():\n",
    "        name = param.name\n",
    "        value = param.data.asnumpy()\n",
    "        print(name, value.shape)\n",
    "        ms_params[name] = value\n",
    "        param_cnt += 1\n",
    "    print(\"total params: \", param_cnt)\n",
    "    return ms_params\n",
    "\n",
    "qFormer2_params = mindspore_params(qFormer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e442d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.embedding_table\n",
      "embedding_table\n",
      "bert.embeddings.word_embeddings.embedding_table\n",
      "before set_input:  bert.embeddings.word_embeddings.embedding_table\n",
      "after set_input, new :  word_embeddings.embedding_table\n",
      "after set_input, self.get_input_embeddings() :  word_embeddings.embedding_table\n",
      "after get_output_embeddings():  word_embeddings.embedding_table\n",
      "word_embeddings.embedding_table\n",
      "word_embeddings.embedding_table\n",
      "word_embeddings.embedding_table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding<vocab_size=30523, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=word_embeddings.embedding_table, shape=(30523, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qFormer2.resize_token_embeddings(30523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e641ad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter (name=cls.predictions.bias, shape=(30523,), dtype=Float32, requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qFormer2.cls.predictions.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e85d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inputs\n",
    "import torch\n",
    "from mindspore import Tensor\n",
    "query_tokens = torch.load(\"/mnt/d/QFormer/query_tokens.pth\", map_location=\"cpu\")\n",
    "image_embeds = torch.load(\"/mnt/d/QFormer/image_embeds.pth\", map_location=\"cpu\")\n",
    "image_atts = torch.load(\"/mnt/d/QFormer/image_atts.pth\", map_location=\"cpu\")\n",
    "\n",
    "\n",
    "query_tokens = Tensor.from_numpy(query_tokens.detach().numpy())\n",
    "image_embeds = Tensor.from_numpy(image_embeds.detach().numpy())\n",
    "image_atts = Tensor.from_numpy(image_atts.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12704a02",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only support assign to attribute of self, but got attribute of cls.\nMore details please refer to syntax support at https://www.mindspore.cn\n\n----------------------------------------------------\n- The Traceback of Net Construct Code:\n----------------------------------------------------\n\n# In file /home/jin/miniconda3/envs/blip2/lib/python3.9/site-packages/mindspore/common/_decorator.py:44\n                cls.substitute_name = substitute\n                ^\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/pipeline/jit/parse/parse.cc:2718 WriteAssignVars\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_output \u001b[38;5;241m=\u001b[39m \u001b[43mqFormer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[43mquery_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_atts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/blip2/lib/python3.9/site-packages/mindspore/nn/cell.py:619\u001b[0m, in \u001b[0;36mCell.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_fn_registered():\n\u001b[1;32m    617\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCell\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not support hook function in graph mode. If you want to use hook \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction, please use context.set_context to set pynative mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 619\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Run in PyNative mode.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/blip2/lib/python3.9/site-packages/mindspore/nn/cell.py:1005\u001b[0m, in \u001b[0;36mCell.compile_and_run\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;124;03mCompile and run Cell, the input must be consistent with the input defined in construct.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m    Object, the result of executing.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_parallel_compile_and_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m new_inputs \u001b[38;5;241m=\u001b[39m _get_args_for_run(\u001b[38;5;28mself\u001b[39m, inputs)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _cell_graph_executor(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mnew_inputs, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase)\n",
      "File \u001b[0;32m~/miniconda3/envs/blip2/lib/python3.9/site-packages/mindspore/nn/cell.py:976\u001b[0m, in \u001b[0;36mCell.compile\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03mCompile Cell as a computation graph, the input must be consistent with the input defined in construct.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    inputs (tuple): Inputs of the Cell object.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_shape_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_shape_inputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     \u001b[43m_cell_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_parallel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_parallel_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mjit_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_compile_dynamic_shape(\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/blip2/lib/python3.9/site-packages/mindspore/common/api.py:1131\u001b[0m, in \u001b[0;36m_CellGraphExecutor.compile\u001b[0;34m(self, obj, phase, do_convert, auto_parallel_mode, jit_config_dict, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jit_config_dict:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_executor\u001b[38;5;241m.\u001b[39mset_jit_config(jit_config_dict)\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_vm_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m obj\u001b[38;5;241m.\u001b[39mcompile_cache\u001b[38;5;241m.\u001b[39madd(phase)\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
      "\u001b[0;31mTypeError\u001b[0m: Only support assign to attribute of self, but got attribute of cls.\nMore details please refer to syntax support at https://www.mindspore.cn\n\n----------------------------------------------------\n- The Traceback of Net Construct Code:\n----------------------------------------------------\n\n# In file /home/jin/miniconda3/envs/blip2/lib/python3.9/site-packages/mindspore/common/_decorator.py:44\n                cls.substitute_name = substitute\n                ^\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/pipeline/jit/parse/parse.cc:2718 WriteAssignVars\n"
     ]
    }
   ],
   "source": [
    "query_output = qFormer2.bert(\n",
    "           query_embeds=query_tokens,\n",
    "           encoder_hidden_states=image_embeds,\n",
    "           encoder_attention_mask=image_atts,\n",
    "           use_cache=True,\n",
    "           return_dict=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf670e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = qFormer2.bert.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            query_embeds=query_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb4a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.position_ids (1, 512)\n",
      "word_embeddings.embedding_table (30523, 768)\n",
      "bert.embeddings.position_embeddings.embedding_table (512, 768)\n",
      "bert.embeddings.LayerNorm.gamma (768,)\n",
      "bert.embeddings.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.0.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.0.crossattention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.0.crossattention.self.query.bias (768,)\n",
      "bert.encoder.layer.0.crossattention.self.key.weight (768, 1408)\n",
      "bert.encoder.layer.0.crossattention.self.key.bias (768,)\n",
      "bert.encoder.layer.0.crossattention.self.value.weight (768, 1408)\n",
      "bert.encoder.layer.0.crossattention.self.value.bias (768,)\n",
      "bert.encoder.layer.0.crossattention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.0.crossattention.output.dense.bias (768,)\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.0.crossattention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.0.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.0.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.0.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.0.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.0.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.0.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.1.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.1.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.1.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.1.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.1.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.1.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.1.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.1.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.1.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.1.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.1.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.1.output.dense.bias (768,)\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.1.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.1.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.1.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.1.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.1.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.1.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.1.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.2.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.2.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.2.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.2.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.2.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.2.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.2.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.2.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.2.crossattention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.2.crossattention.self.query.bias (768,)\n",
      "bert.encoder.layer.2.crossattention.self.key.weight (768, 1408)\n",
      "bert.encoder.layer.2.crossattention.self.key.bias (768,)\n",
      "bert.encoder.layer.2.crossattention.self.value.weight (768, 1408)\n",
      "bert.encoder.layer.2.crossattention.self.value.bias (768,)\n",
      "bert.encoder.layer.2.crossattention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.2.crossattention.output.dense.bias (768,)\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.2.crossattention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.2.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.2.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.2.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.2.output.dense.bias (768,)\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.2.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.2.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.2.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.2.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.2.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.2.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.2.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.3.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.3.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.3.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.3.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.3.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.3.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.3.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.3.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.3.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.3.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.3.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.3.output.dense.bias (768,)\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.3.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.3.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.3.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.3.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.3.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.3.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.3.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.4.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.4.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.4.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.4.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.4.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.4.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.4.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.4.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.4.crossattention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.4.crossattention.self.query.bias (768,)\n",
      "bert.encoder.layer.4.crossattention.self.key.weight (768, 1408)\n",
      "bert.encoder.layer.4.crossattention.self.key.bias (768,)\n",
      "bert.encoder.layer.4.crossattention.self.value.weight (768, 1408)\n",
      "bert.encoder.layer.4.crossattention.self.value.bias (768,)\n",
      "bert.encoder.layer.4.crossattention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.4.crossattention.output.dense.bias (768,)\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.4.crossattention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.4.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.4.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.4.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.4.output.dense.bias (768,)\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.4.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.4.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.4.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.4.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.4.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.4.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.4.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.5.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.5.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.5.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.5.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.5.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.5.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.5.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.5.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.5.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.5.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.5.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.5.output.dense.bias (768,)\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.5.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.5.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.5.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.5.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.5.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.5.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.5.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.6.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.6.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.6.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.6.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.6.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.6.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.6.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.6.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.6.crossattention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.6.crossattention.self.query.bias (768,)\n",
      "bert.encoder.layer.6.crossattention.self.key.weight (768, 1408)\n",
      "bert.encoder.layer.6.crossattention.self.key.bias (768,)\n",
      "bert.encoder.layer.6.crossattention.self.value.weight (768, 1408)\n",
      "bert.encoder.layer.6.crossattention.self.value.bias (768,)\n",
      "bert.encoder.layer.6.crossattention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.6.crossattention.output.dense.bias (768,)\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.6.crossattention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.6.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.6.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.6.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.6.output.dense.bias (768,)\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.6.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.6.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.6.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.6.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.6.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.6.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.6.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.7.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.7.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.7.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.7.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.7.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.7.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.7.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.7.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.7.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.7.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.7.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.7.output.dense.bias (768,)\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.7.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.7.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.7.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.7.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.7.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.7.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.7.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.8.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.8.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.8.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.8.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.8.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.8.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.8.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.8.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.8.crossattention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.8.crossattention.self.query.bias (768,)\n",
      "bert.encoder.layer.8.crossattention.self.key.weight (768, 1408)\n",
      "bert.encoder.layer.8.crossattention.self.key.bias (768,)\n",
      "bert.encoder.layer.8.crossattention.self.value.weight (768, 1408)\n",
      "bert.encoder.layer.8.crossattention.self.value.bias (768,)\n",
      "bert.encoder.layer.8.crossattention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.8.crossattention.output.dense.bias (768,)\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.8.crossattention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.8.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.8.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.8.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.8.output.dense.bias (768,)\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.8.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.8.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.8.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.8.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.8.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.8.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.8.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.9.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.9.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.9.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.9.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.9.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.9.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.9.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.9.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.9.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.9.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.9.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.9.output.dense.bias (768,)\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.9.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.9.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.9.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.9.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.9.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.9.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.9.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.10.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.10.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.10.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.10.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.10.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.10.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.10.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.10.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.10.crossattention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.10.crossattention.self.query.bias (768,)\n",
      "bert.encoder.layer.10.crossattention.self.key.weight (768, 1408)\n",
      "bert.encoder.layer.10.crossattention.self.key.bias (768,)\n",
      "bert.encoder.layer.10.crossattention.self.value.weight (768, 1408)\n",
      "bert.encoder.layer.10.crossattention.self.value.bias (768,)\n",
      "bert.encoder.layer.10.crossattention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.10.crossattention.output.dense.bias (768,)\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.10.crossattention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.10.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.10.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.10.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.10.output.dense.bias (768,)\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.10.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.10.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.10.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.10.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.10.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.10.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.10.output_query.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.11.attention.self.query.weight (768, 768)\n",
      "bert.encoder.layer.11.attention.self.query.bias (768,)\n",
      "bert.encoder.layer.11.attention.self.key.weight (768, 768)\n",
      "bert.encoder.layer.11.attention.self.key.bias (768,)\n",
      "bert.encoder.layer.11.attention.self.value.weight (768, 768)\n",
      "bert.encoder.layer.11.attention.self.value.bias (768,)\n",
      "bert.encoder.layer.11.attention.output.dense.weight (768, 768)\n",
      "bert.encoder.layer.11.attention.output.dense.bias (768,)\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.11.intermediate.dense.weight (3072, 768)\n",
      "bert.encoder.layer.11.intermediate.dense.bias (3072,)\n",
      "bert.encoder.layer.11.output.dense.weight (768, 3072)\n",
      "bert.encoder.layer.11.output.dense.bias (768,)\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.11.output.LayerNorm.beta (768,)\n",
      "bert.encoder.layer.11.intermediate_query.dense.weight (3072, 768)\n",
      "bert.encoder.layer.11.intermediate_query.dense.bias (3072,)\n",
      "bert.encoder.layer.11.output_query.dense.weight (768, 3072)\n",
      "bert.encoder.layer.11.output_query.dense.bias (768,)\n",
      "bert.encoder.layer.11.output_query.LayerNorm.gamma (768,)\n",
      "bert.encoder.layer.11.output_query.LayerNorm.beta (768,)\n",
      "cls.predictions.bias (30523,)\n",
      "cls.predictions.transform.dense.weight (768, 768)\n",
      "cls.predictions.transform.dense.bias (768,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls.predictions.transform.LayerNorm.gamma (768,)\n",
      "cls.predictions.transform.LayerNorm.beta (768,)\n",
      "cls.predictions.decoder.weight (30523, 768)\n",
      "total params:  335\n"
     ]
    }
   ],
   "source": [
    "qFormer2_params = mindspore_params(qFormer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7ed6730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainedConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"add_cross_attention\": true,\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"cross_attention_freq\": 2,\n",
       "  \"encoder_width\": 1408,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"query_length\": 32,\n",
       "  \"transformers_version\": \"4.25.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30523\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qFormer2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f0050c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnlyMLMHead<\n",
       "  (predictions): BertLMPredictionHead<\n",
       "    (transform): BertPredictionHeadTransform<\n",
       "      (dense): Mock_Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "      (transform_act_fn): IdentityACT<>\n",
       "      (LayerNorm): LayerNorm<normalized_shape=[768], begin_norm_axis=-1, begin_params_axis=-1, gammaParameter (name=cls.predictions.transform.LayerNorm.gamma, shape=(768,), dtype=Float32, requires_grad=True), beta=Parameter (name=cls.predictions.transform.LayerNorm.beta, shape=(768,), dtype=Float32, requires_grad=True)>\n",
       "      >\n",
       "    (decoder): Mock_Dense<input_channels=768, output_channels=30522>\n",
       "    >\n",
       "  >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qFormer2.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa488dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qFormer2_params = mindspore_params(qFormer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import numpy as np\n",
    "from mindspore import Parameter\n",
    "a = np.randn(4, 2, 4)\n",
    "b = np.randn(4, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Parameter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.assign_value(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2_params = mindspore_params(qFormer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2.get_output_embeddings().weight.value().ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops as P\n",
    "from mindspore import numpy as np\n",
    "xx = np.arange(20).reshape(2, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = P.expand_dims(xx, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e37ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxx = P.broadcast_to(xxx, (2, -1, -1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xxxx[0] == xxxx[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = np.arange(20).reshape(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy = P.expand_dims(P.expand_dims(yy, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(yyy == (yy[:, None, None, :])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba048b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4307392",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy.transpose(2, 1, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eebd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt1 = P.expand_dims(yy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a686c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec337b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt2 = yt1.copy()\n",
    "yt2 = torch.from_numpy(yt2.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt2.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_shape = [ i for i in range(yt1.dim())]\n",
    "tmp_shape[-1], tmp_shape[-2] = tmp_shape[-2], tmp_shape[-1]\n",
    "# tmp_shape = tuple(tmp_shape)\n",
    "print(tmp_shape)\n",
    "yt1.transpose(tmp_shape).asnumpy() == yt2.transpose(-1, -2).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48229833",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output = qFormer2.bert(\n",
    "           query_embeds=query_tokens,\n",
    "           encoder_hidden_states=image_embeds,\n",
    "           encoder_attention_mask=image_atts,\n",
    "           use_cache=True,\n",
    "           return_dict=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4894ced",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------ split line ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00622706",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = qFormer2.get_input_embeddings().embedding_table.value().dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "a = nn.Cell()\n",
    "a.cells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d269b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(f: nn.Cell):\n",
    "    print(f)\n",
    "a.apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_a(value):\n",
    "    value = None\n",
    "    return\n",
    "\n",
    "a = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_a(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = qFormer2.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6eb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = embed.embedding_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5dbbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense = qFormer2.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m = torch.nn.LayerNorm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de925ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn\n",
    "x = nn.LayerNorm([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50023b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ops as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840a588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.gamma.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d174b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.gamma.init_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.gamma.set_data(P.zeros_like(x.gamma.value()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b151aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2.config.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qFormer2.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(qFormer2.get_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2.cls.predictions.bias.has_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2_params = mindspore_params(qFormer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total params: \", len(state_dict))\n",
    "for name, tensor in state_dict.items():\n",
    "    print(name, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in qFormer2.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699db64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12bb049b",
   "metadata": {},
   "source": [
    "#### convert and save state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mindspore_params(network):\n",
    "    ms_params = {}\n",
    "    param_cnt = 0\n",
    "    for param in network.get_parameters():\n",
    "        name = param.name\n",
    "        value = param.data.value()\n",
    "        print(name, value.shape)\n",
    "        ms_params[name] = value\n",
    "        param_cnt += 1\n",
    "    print(\"total params: \", param_cnt)\n",
    "    return ms_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in qFormer2.get_parameters():\n",
    "    print(param.name, param.data.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_params = mindspore_params(qFormer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d756bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pt_params = torch.load(\"/mnt/d/QFormer/self_Qformer_state_dict_init.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132b29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "def param_convert(ms_params, pt_params, ckpt_path):\n",
    "    # 参数名映射字典\n",
    "    bn_ms2pt = {\"gamma\": \"weight\",\n",
    "                \"beta\": \"bias\",\n",
    "                \"embedding_table\": \"weight\"}\n",
    "    new_params_list = []\n",
    "    for ms_param in ms_params.keys():\n",
    "        # 规整化 nn.Embedding和LayerNrom的参数名称\n",
    "        replace_flag = False\n",
    "        for replace_word in bn_ms2pt:\n",
    "            if replace_word in ms_param:\n",
    "                   replace_flag = True\n",
    "        if replace_flag:\n",
    "            ms_param_item = ms_param.split(\".\")\n",
    "            pt_param_item = ms_param_item[:-1] + [bn_ms2pt[ms_param_item[-1]]]\n",
    "            pt_param = \".\".join(pt_param_item)\n",
    "            if pt_param in pt_params and pt_params[pt_param].shape == ms_params[ms_param].shape:\n",
    "                numpy_value = pt_params[pt_param].detach().numpy()\n",
    "                new_params_list.append({\"name\": ms_param, \"data\": Tensor.from_numpy(numpy_value)})\n",
    "                print(\"loading {} from {}\".format(ms_param, pt_param))\n",
    "            else:\n",
    "                print(ms_param, \"not match in pt_params\")\n",
    "        else:\n",
    "            if ms_param in pt_params and pt_params[ms_param].shape == ms_params[ms_param].shape:\n",
    "                numpy_value = pt_params[ms_param].detach().numpy()\n",
    "                new_params_list.append({\"name\": ms_param, \"data\": Tensor.from_numpy(numpy_value)})\n",
    "                print(\"loading {} from {}\".format(ms_param, ms_param))\n",
    "            else:\n",
    "                print(ms_param, \"not match in pt_params\")\n",
    "    ms.save_checkpoint(new_params_list, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17c78cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading bert.embeddings.position_ids from bert.embeddings.position_ids\n",
      "word_embeddings.embedding_table not match in pt_params\n",
      "loading bert.embeddings.position_embeddings.embedding_table from bert.embeddings.position_embeddings.weight\n",
      "loading bert.embeddings.LayerNorm.gamma from bert.embeddings.LayerNorm.weight\n",
      "loading bert.embeddings.LayerNorm.beta from bert.embeddings.LayerNorm.bias\n",
      "loading bert.encoder.layer.0.attention.self.query.weight from bert.encoder.layer.0.attention.self.query.weight\n",
      "loading bert.encoder.layer.0.attention.self.query.bias from bert.encoder.layer.0.attention.self.query.bias\n",
      "loading bert.encoder.layer.0.attention.self.key.weight from bert.encoder.layer.0.attention.self.key.weight\n",
      "loading bert.encoder.layer.0.attention.self.key.bias from bert.encoder.layer.0.attention.self.key.bias\n",
      "loading bert.encoder.layer.0.attention.self.value.weight from bert.encoder.layer.0.attention.self.value.weight\n",
      "loading bert.encoder.layer.0.attention.self.value.bias from bert.encoder.layer.0.attention.self.value.bias\n",
      "loading bert.encoder.layer.0.attention.output.dense.weight from bert.encoder.layer.0.attention.output.dense.weight\n",
      "loading bert.encoder.layer.0.attention.output.dense.bias from bert.encoder.layer.0.attention.output.dense.bias\n",
      "loading bert.encoder.layer.0.attention.output.LayerNorm.gamma from bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.0.attention.output.LayerNorm.beta from bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.0.crossattention.self.query.weight from bert.encoder.layer.0.crossattention.self.query.weight\n",
      "loading bert.encoder.layer.0.crossattention.self.query.bias from bert.encoder.layer.0.crossattention.self.query.bias\n",
      "loading bert.encoder.layer.0.crossattention.self.key.weight from bert.encoder.layer.0.crossattention.self.key.weight\n",
      "loading bert.encoder.layer.0.crossattention.self.key.bias from bert.encoder.layer.0.crossattention.self.key.bias\n",
      "loading bert.encoder.layer.0.crossattention.self.value.weight from bert.encoder.layer.0.crossattention.self.value.weight\n",
      "loading bert.encoder.layer.0.crossattention.self.value.bias from bert.encoder.layer.0.crossattention.self.value.bias\n",
      "loading bert.encoder.layer.0.crossattention.output.dense.weight from bert.encoder.layer.0.crossattention.output.dense.weight\n",
      "loading bert.encoder.layer.0.crossattention.output.dense.bias from bert.encoder.layer.0.crossattention.output.dense.bias\n",
      "loading bert.encoder.layer.0.crossattention.output.LayerNorm.gamma from bert.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.0.crossattention.output.LayerNorm.beta from bert.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.0.intermediate.dense.weight from bert.encoder.layer.0.intermediate.dense.weight\n",
      "loading bert.encoder.layer.0.intermediate.dense.bias from bert.encoder.layer.0.intermediate.dense.bias\n",
      "loading bert.encoder.layer.0.output.dense.weight from bert.encoder.layer.0.output.dense.weight\n",
      "loading bert.encoder.layer.0.output.dense.bias from bert.encoder.layer.0.output.dense.bias\n",
      "loading bert.encoder.layer.0.output.LayerNorm.gamma from bert.encoder.layer.0.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.0.output.LayerNorm.beta from bert.encoder.layer.0.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.0.intermediate_query.dense.weight from bert.encoder.layer.0.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.0.intermediate_query.dense.bias from bert.encoder.layer.0.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.0.output_query.dense.weight from bert.encoder.layer.0.output_query.dense.weight\n",
      "loading bert.encoder.layer.0.output_query.dense.bias from bert.encoder.layer.0.output_query.dense.bias\n",
      "loading bert.encoder.layer.0.output_query.LayerNorm.gamma from bert.encoder.layer.0.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.0.output_query.LayerNorm.beta from bert.encoder.layer.0.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.1.attention.self.query.weight from bert.encoder.layer.1.attention.self.query.weight\n",
      "loading bert.encoder.layer.1.attention.self.query.bias from bert.encoder.layer.1.attention.self.query.bias\n",
      "loading bert.encoder.layer.1.attention.self.key.weight from bert.encoder.layer.1.attention.self.key.weight\n",
      "loading bert.encoder.layer.1.attention.self.key.bias from bert.encoder.layer.1.attention.self.key.bias\n",
      "loading bert.encoder.layer.1.attention.self.value.weight from bert.encoder.layer.1.attention.self.value.weight\n",
      "loading bert.encoder.layer.1.attention.self.value.bias from bert.encoder.layer.1.attention.self.value.bias\n",
      "loading bert.encoder.layer.1.attention.output.dense.weight from bert.encoder.layer.1.attention.output.dense.weight\n",
      "loading bert.encoder.layer.1.attention.output.dense.bias from bert.encoder.layer.1.attention.output.dense.bias\n",
      "loading bert.encoder.layer.1.attention.output.LayerNorm.gamma from bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.1.attention.output.LayerNorm.beta from bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.1.intermediate.dense.weight from bert.encoder.layer.1.intermediate.dense.weight\n",
      "loading bert.encoder.layer.1.intermediate.dense.bias from bert.encoder.layer.1.intermediate.dense.bias\n",
      "loading bert.encoder.layer.1.output.dense.weight from bert.encoder.layer.1.output.dense.weight\n",
      "loading bert.encoder.layer.1.output.dense.bias from bert.encoder.layer.1.output.dense.bias\n",
      "loading bert.encoder.layer.1.output.LayerNorm.gamma from bert.encoder.layer.1.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.1.output.LayerNorm.beta from bert.encoder.layer.1.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.1.intermediate_query.dense.weight from bert.encoder.layer.1.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.1.intermediate_query.dense.bias from bert.encoder.layer.1.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.1.output_query.dense.weight from bert.encoder.layer.1.output_query.dense.weight\n",
      "loading bert.encoder.layer.1.output_query.dense.bias from bert.encoder.layer.1.output_query.dense.bias\n",
      "loading bert.encoder.layer.1.output_query.LayerNorm.gamma from bert.encoder.layer.1.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.1.output_query.LayerNorm.beta from bert.encoder.layer.1.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.2.attention.self.query.weight from bert.encoder.layer.2.attention.self.query.weight\n",
      "loading bert.encoder.layer.2.attention.self.query.bias from bert.encoder.layer.2.attention.self.query.bias\n",
      "loading bert.encoder.layer.2.attention.self.key.weight from bert.encoder.layer.2.attention.self.key.weight\n",
      "loading bert.encoder.layer.2.attention.self.key.bias from bert.encoder.layer.2.attention.self.key.bias\n",
      "loading bert.encoder.layer.2.attention.self.value.weight from bert.encoder.layer.2.attention.self.value.weight\n",
      "loading bert.encoder.layer.2.attention.self.value.bias from bert.encoder.layer.2.attention.self.value.bias\n",
      "loading bert.encoder.layer.2.attention.output.dense.weight from bert.encoder.layer.2.attention.output.dense.weight\n",
      "loading bert.encoder.layer.2.attention.output.dense.bias from bert.encoder.layer.2.attention.output.dense.bias\n",
      "loading bert.encoder.layer.2.attention.output.LayerNorm.gamma from bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.2.attention.output.LayerNorm.beta from bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.2.crossattention.self.query.weight from bert.encoder.layer.2.crossattention.self.query.weight\n",
      "loading bert.encoder.layer.2.crossattention.self.query.bias from bert.encoder.layer.2.crossattention.self.query.bias\n",
      "loading bert.encoder.layer.2.crossattention.self.key.weight from bert.encoder.layer.2.crossattention.self.key.weight\n",
      "loading bert.encoder.layer.2.crossattention.self.key.bias from bert.encoder.layer.2.crossattention.self.key.bias\n",
      "loading bert.encoder.layer.2.crossattention.self.value.weight from bert.encoder.layer.2.crossattention.self.value.weight\n",
      "loading bert.encoder.layer.2.crossattention.self.value.bias from bert.encoder.layer.2.crossattention.self.value.bias\n",
      "loading bert.encoder.layer.2.crossattention.output.dense.weight from bert.encoder.layer.2.crossattention.output.dense.weight\n",
      "loading bert.encoder.layer.2.crossattention.output.dense.bias from bert.encoder.layer.2.crossattention.output.dense.bias\n",
      "loading bert.encoder.layer.2.crossattention.output.LayerNorm.gamma from bert.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.2.crossattention.output.LayerNorm.beta from bert.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.2.intermediate.dense.weight from bert.encoder.layer.2.intermediate.dense.weight\n",
      "loading bert.encoder.layer.2.intermediate.dense.bias from bert.encoder.layer.2.intermediate.dense.bias\n",
      "loading bert.encoder.layer.2.output.dense.weight from bert.encoder.layer.2.output.dense.weight\n",
      "loading bert.encoder.layer.2.output.dense.bias from bert.encoder.layer.2.output.dense.bias\n",
      "loading bert.encoder.layer.2.output.LayerNorm.gamma from bert.encoder.layer.2.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.2.output.LayerNorm.beta from bert.encoder.layer.2.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.2.intermediate_query.dense.weight from bert.encoder.layer.2.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.2.intermediate_query.dense.bias from bert.encoder.layer.2.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.2.output_query.dense.weight from bert.encoder.layer.2.output_query.dense.weight\n",
      "loading bert.encoder.layer.2.output_query.dense.bias from bert.encoder.layer.2.output_query.dense.bias\n",
      "loading bert.encoder.layer.2.output_query.LayerNorm.gamma from bert.encoder.layer.2.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.2.output_query.LayerNorm.beta from bert.encoder.layer.2.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.3.attention.self.query.weight from bert.encoder.layer.3.attention.self.query.weight\n",
      "loading bert.encoder.layer.3.attention.self.query.bias from bert.encoder.layer.3.attention.self.query.bias\n",
      "loading bert.encoder.layer.3.attention.self.key.weight from bert.encoder.layer.3.attention.self.key.weight\n",
      "loading bert.encoder.layer.3.attention.self.key.bias from bert.encoder.layer.3.attention.self.key.bias\n",
      "loading bert.encoder.layer.3.attention.self.value.weight from bert.encoder.layer.3.attention.self.value.weight\n",
      "loading bert.encoder.layer.3.attention.self.value.bias from bert.encoder.layer.3.attention.self.value.bias\n",
      "loading bert.encoder.layer.3.attention.output.dense.weight from bert.encoder.layer.3.attention.output.dense.weight\n",
      "loading bert.encoder.layer.3.attention.output.dense.bias from bert.encoder.layer.3.attention.output.dense.bias\n",
      "loading bert.encoder.layer.3.attention.output.LayerNorm.gamma from bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.3.attention.output.LayerNorm.beta from bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.3.intermediate.dense.weight from bert.encoder.layer.3.intermediate.dense.weight\n",
      "loading bert.encoder.layer.3.intermediate.dense.bias from bert.encoder.layer.3.intermediate.dense.bias\n",
      "loading bert.encoder.layer.3.output.dense.weight from bert.encoder.layer.3.output.dense.weight\n",
      "loading bert.encoder.layer.3.output.dense.bias from bert.encoder.layer.3.output.dense.bias\n",
      "loading bert.encoder.layer.3.output.LayerNorm.gamma from bert.encoder.layer.3.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.3.output.LayerNorm.beta from bert.encoder.layer.3.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.3.intermediate_query.dense.weight from bert.encoder.layer.3.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.3.intermediate_query.dense.bias from bert.encoder.layer.3.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.3.output_query.dense.weight from bert.encoder.layer.3.output_query.dense.weight\n",
      "loading bert.encoder.layer.3.output_query.dense.bias from bert.encoder.layer.3.output_query.dense.bias\n",
      "loading bert.encoder.layer.3.output_query.LayerNorm.gamma from bert.encoder.layer.3.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.3.output_query.LayerNorm.beta from bert.encoder.layer.3.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.4.attention.self.query.weight from bert.encoder.layer.4.attention.self.query.weight\n",
      "loading bert.encoder.layer.4.attention.self.query.bias from bert.encoder.layer.4.attention.self.query.bias\n",
      "loading bert.encoder.layer.4.attention.self.key.weight from bert.encoder.layer.4.attention.self.key.weight\n",
      "loading bert.encoder.layer.4.attention.self.key.bias from bert.encoder.layer.4.attention.self.key.bias\n",
      "loading bert.encoder.layer.4.attention.self.value.weight from bert.encoder.layer.4.attention.self.value.weight\n",
      "loading bert.encoder.layer.4.attention.self.value.bias from bert.encoder.layer.4.attention.self.value.bias\n",
      "loading bert.encoder.layer.4.attention.output.dense.weight from bert.encoder.layer.4.attention.output.dense.weight\n",
      "loading bert.encoder.layer.4.attention.output.dense.bias from bert.encoder.layer.4.attention.output.dense.bias\n",
      "loading bert.encoder.layer.4.attention.output.LayerNorm.gamma from bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.4.attention.output.LayerNorm.beta from bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.4.crossattention.self.query.weight from bert.encoder.layer.4.crossattention.self.query.weight\n",
      "loading bert.encoder.layer.4.crossattention.self.query.bias from bert.encoder.layer.4.crossattention.self.query.bias\n",
      "loading bert.encoder.layer.4.crossattention.self.key.weight from bert.encoder.layer.4.crossattention.self.key.weight\n",
      "loading bert.encoder.layer.4.crossattention.self.key.bias from bert.encoder.layer.4.crossattention.self.key.bias\n",
      "loading bert.encoder.layer.4.crossattention.self.value.weight from bert.encoder.layer.4.crossattention.self.value.weight\n",
      "loading bert.encoder.layer.4.crossattention.self.value.bias from bert.encoder.layer.4.crossattention.self.value.bias\n",
      "loading bert.encoder.layer.4.crossattention.output.dense.weight from bert.encoder.layer.4.crossattention.output.dense.weight\n",
      "loading bert.encoder.layer.4.crossattention.output.dense.bias from bert.encoder.layer.4.crossattention.output.dense.bias\n",
      "loading bert.encoder.layer.4.crossattention.output.LayerNorm.gamma from bert.encoder.layer.4.crossattention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.4.crossattention.output.LayerNorm.beta from bert.encoder.layer.4.crossattention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.4.intermediate.dense.weight from bert.encoder.layer.4.intermediate.dense.weight\n",
      "loading bert.encoder.layer.4.intermediate.dense.bias from bert.encoder.layer.4.intermediate.dense.bias\n",
      "loading bert.encoder.layer.4.output.dense.weight from bert.encoder.layer.4.output.dense.weight\n",
      "loading bert.encoder.layer.4.output.dense.bias from bert.encoder.layer.4.output.dense.bias\n",
      "loading bert.encoder.layer.4.output.LayerNorm.gamma from bert.encoder.layer.4.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.4.output.LayerNorm.beta from bert.encoder.layer.4.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.4.intermediate_query.dense.weight from bert.encoder.layer.4.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.4.intermediate_query.dense.bias from bert.encoder.layer.4.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.4.output_query.dense.weight from bert.encoder.layer.4.output_query.dense.weight\n",
      "loading bert.encoder.layer.4.output_query.dense.bias from bert.encoder.layer.4.output_query.dense.bias\n",
      "loading bert.encoder.layer.4.output_query.LayerNorm.gamma from bert.encoder.layer.4.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.4.output_query.LayerNorm.beta from bert.encoder.layer.4.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.5.attention.self.query.weight from bert.encoder.layer.5.attention.self.query.weight\n",
      "loading bert.encoder.layer.5.attention.self.query.bias from bert.encoder.layer.5.attention.self.query.bias\n",
      "loading bert.encoder.layer.5.attention.self.key.weight from bert.encoder.layer.5.attention.self.key.weight\n",
      "loading bert.encoder.layer.5.attention.self.key.bias from bert.encoder.layer.5.attention.self.key.bias\n",
      "loading bert.encoder.layer.5.attention.self.value.weight from bert.encoder.layer.5.attention.self.value.weight\n",
      "loading bert.encoder.layer.5.attention.self.value.bias from bert.encoder.layer.5.attention.self.value.bias\n",
      "loading bert.encoder.layer.5.attention.output.dense.weight from bert.encoder.layer.5.attention.output.dense.weight\n",
      "loading bert.encoder.layer.5.attention.output.dense.bias from bert.encoder.layer.5.attention.output.dense.bias\n",
      "loading bert.encoder.layer.5.attention.output.LayerNorm.gamma from bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.5.attention.output.LayerNorm.beta from bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.5.intermediate.dense.weight from bert.encoder.layer.5.intermediate.dense.weight\n",
      "loading bert.encoder.layer.5.intermediate.dense.bias from bert.encoder.layer.5.intermediate.dense.bias\n",
      "loading bert.encoder.layer.5.output.dense.weight from bert.encoder.layer.5.output.dense.weight\n",
      "loading bert.encoder.layer.5.output.dense.bias from bert.encoder.layer.5.output.dense.bias\n",
      "loading bert.encoder.layer.5.output.LayerNorm.gamma from bert.encoder.layer.5.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.5.output.LayerNorm.beta from bert.encoder.layer.5.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.5.intermediate_query.dense.weight from bert.encoder.layer.5.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.5.intermediate_query.dense.bias from bert.encoder.layer.5.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.5.output_query.dense.weight from bert.encoder.layer.5.output_query.dense.weight\n",
      "loading bert.encoder.layer.5.output_query.dense.bias from bert.encoder.layer.5.output_query.dense.bias\n",
      "loading bert.encoder.layer.5.output_query.LayerNorm.gamma from bert.encoder.layer.5.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.5.output_query.LayerNorm.beta from bert.encoder.layer.5.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.6.attention.self.query.weight from bert.encoder.layer.6.attention.self.query.weight\n",
      "loading bert.encoder.layer.6.attention.self.query.bias from bert.encoder.layer.6.attention.self.query.bias\n",
      "loading bert.encoder.layer.6.attention.self.key.weight from bert.encoder.layer.6.attention.self.key.weight\n",
      "loading bert.encoder.layer.6.attention.self.key.bias from bert.encoder.layer.6.attention.self.key.bias\n",
      "loading bert.encoder.layer.6.attention.self.value.weight from bert.encoder.layer.6.attention.self.value.weight\n",
      "loading bert.encoder.layer.6.attention.self.value.bias from bert.encoder.layer.6.attention.self.value.bias\n",
      "loading bert.encoder.layer.6.attention.output.dense.weight from bert.encoder.layer.6.attention.output.dense.weight\n",
      "loading bert.encoder.layer.6.attention.output.dense.bias from bert.encoder.layer.6.attention.output.dense.bias\n",
      "loading bert.encoder.layer.6.attention.output.LayerNorm.gamma from bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.6.attention.output.LayerNorm.beta from bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.6.crossattention.self.query.weight from bert.encoder.layer.6.crossattention.self.query.weight\n",
      "loading bert.encoder.layer.6.crossattention.self.query.bias from bert.encoder.layer.6.crossattention.self.query.bias\n",
      "loading bert.encoder.layer.6.crossattention.self.key.weight from bert.encoder.layer.6.crossattention.self.key.weight\n",
      "loading bert.encoder.layer.6.crossattention.self.key.bias from bert.encoder.layer.6.crossattention.self.key.bias\n",
      "loading bert.encoder.layer.6.crossattention.self.value.weight from bert.encoder.layer.6.crossattention.self.value.weight\n",
      "loading bert.encoder.layer.6.crossattention.self.value.bias from bert.encoder.layer.6.crossattention.self.value.bias\n",
      "loading bert.encoder.layer.6.crossattention.output.dense.weight from bert.encoder.layer.6.crossattention.output.dense.weight\n",
      "loading bert.encoder.layer.6.crossattention.output.dense.bias from bert.encoder.layer.6.crossattention.output.dense.bias\n",
      "loading bert.encoder.layer.6.crossattention.output.LayerNorm.gamma from bert.encoder.layer.6.crossattention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.6.crossattention.output.LayerNorm.beta from bert.encoder.layer.6.crossattention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.6.intermediate.dense.weight from bert.encoder.layer.6.intermediate.dense.weight\n",
      "loading bert.encoder.layer.6.intermediate.dense.bias from bert.encoder.layer.6.intermediate.dense.bias\n",
      "loading bert.encoder.layer.6.output.dense.weight from bert.encoder.layer.6.output.dense.weight\n",
      "loading bert.encoder.layer.6.output.dense.bias from bert.encoder.layer.6.output.dense.bias\n",
      "loading bert.encoder.layer.6.output.LayerNorm.gamma from bert.encoder.layer.6.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.6.output.LayerNorm.beta from bert.encoder.layer.6.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.6.intermediate_query.dense.weight from bert.encoder.layer.6.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.6.intermediate_query.dense.bias from bert.encoder.layer.6.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.6.output_query.dense.weight from bert.encoder.layer.6.output_query.dense.weight\n",
      "loading bert.encoder.layer.6.output_query.dense.bias from bert.encoder.layer.6.output_query.dense.bias\n",
      "loading bert.encoder.layer.6.output_query.LayerNorm.gamma from bert.encoder.layer.6.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.6.output_query.LayerNorm.beta from bert.encoder.layer.6.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.7.attention.self.query.weight from bert.encoder.layer.7.attention.self.query.weight\n",
      "loading bert.encoder.layer.7.attention.self.query.bias from bert.encoder.layer.7.attention.self.query.bias\n",
      "loading bert.encoder.layer.7.attention.self.key.weight from bert.encoder.layer.7.attention.self.key.weight\n",
      "loading bert.encoder.layer.7.attention.self.key.bias from bert.encoder.layer.7.attention.self.key.bias\n",
      "loading bert.encoder.layer.7.attention.self.value.weight from bert.encoder.layer.7.attention.self.value.weight\n",
      "loading bert.encoder.layer.7.attention.self.value.bias from bert.encoder.layer.7.attention.self.value.bias\n",
      "loading bert.encoder.layer.7.attention.output.dense.weight from bert.encoder.layer.7.attention.output.dense.weight\n",
      "loading bert.encoder.layer.7.attention.output.dense.bias from bert.encoder.layer.7.attention.output.dense.bias\n",
      "loading bert.encoder.layer.7.attention.output.LayerNorm.gamma from bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.7.attention.output.LayerNorm.beta from bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.7.intermediate.dense.weight from bert.encoder.layer.7.intermediate.dense.weight\n",
      "loading bert.encoder.layer.7.intermediate.dense.bias from bert.encoder.layer.7.intermediate.dense.bias\n",
      "loading bert.encoder.layer.7.output.dense.weight from bert.encoder.layer.7.output.dense.weight\n",
      "loading bert.encoder.layer.7.output.dense.bias from bert.encoder.layer.7.output.dense.bias\n",
      "loading bert.encoder.layer.7.output.LayerNorm.gamma from bert.encoder.layer.7.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.7.output.LayerNorm.beta from bert.encoder.layer.7.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.7.intermediate_query.dense.weight from bert.encoder.layer.7.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.7.intermediate_query.dense.bias from bert.encoder.layer.7.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.7.output_query.dense.weight from bert.encoder.layer.7.output_query.dense.weight\n",
      "loading bert.encoder.layer.7.output_query.dense.bias from bert.encoder.layer.7.output_query.dense.bias\n",
      "loading bert.encoder.layer.7.output_query.LayerNorm.gamma from bert.encoder.layer.7.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.7.output_query.LayerNorm.beta from bert.encoder.layer.7.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.8.attention.self.query.weight from bert.encoder.layer.8.attention.self.query.weight\n",
      "loading bert.encoder.layer.8.attention.self.query.bias from bert.encoder.layer.8.attention.self.query.bias\n",
      "loading bert.encoder.layer.8.attention.self.key.weight from bert.encoder.layer.8.attention.self.key.weight\n",
      "loading bert.encoder.layer.8.attention.self.key.bias from bert.encoder.layer.8.attention.self.key.bias\n",
      "loading bert.encoder.layer.8.attention.self.value.weight from bert.encoder.layer.8.attention.self.value.weight\n",
      "loading bert.encoder.layer.8.attention.self.value.bias from bert.encoder.layer.8.attention.self.value.bias\n",
      "loading bert.encoder.layer.8.attention.output.dense.weight from bert.encoder.layer.8.attention.output.dense.weight\n",
      "loading bert.encoder.layer.8.attention.output.dense.bias from bert.encoder.layer.8.attention.output.dense.bias\n",
      "loading bert.encoder.layer.8.attention.output.LayerNorm.gamma from bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.8.attention.output.LayerNorm.beta from bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.8.crossattention.self.query.weight from bert.encoder.layer.8.crossattention.self.query.weight\n",
      "loading bert.encoder.layer.8.crossattention.self.query.bias from bert.encoder.layer.8.crossattention.self.query.bias\n",
      "loading bert.encoder.layer.8.crossattention.self.key.weight from bert.encoder.layer.8.crossattention.self.key.weight\n",
      "loading bert.encoder.layer.8.crossattention.self.key.bias from bert.encoder.layer.8.crossattention.self.key.bias\n",
      "loading bert.encoder.layer.8.crossattention.self.value.weight from bert.encoder.layer.8.crossattention.self.value.weight\n",
      "loading bert.encoder.layer.8.crossattention.self.value.bias from bert.encoder.layer.8.crossattention.self.value.bias\n",
      "loading bert.encoder.layer.8.crossattention.output.dense.weight from bert.encoder.layer.8.crossattention.output.dense.weight\n",
      "loading bert.encoder.layer.8.crossattention.output.dense.bias from bert.encoder.layer.8.crossattention.output.dense.bias\n",
      "loading bert.encoder.layer.8.crossattention.output.LayerNorm.gamma from bert.encoder.layer.8.crossattention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.8.crossattention.output.LayerNorm.beta from bert.encoder.layer.8.crossattention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.8.intermediate.dense.weight from bert.encoder.layer.8.intermediate.dense.weight\n",
      "loading bert.encoder.layer.8.intermediate.dense.bias from bert.encoder.layer.8.intermediate.dense.bias\n",
      "loading bert.encoder.layer.8.output.dense.weight from bert.encoder.layer.8.output.dense.weight\n",
      "loading bert.encoder.layer.8.output.dense.bias from bert.encoder.layer.8.output.dense.bias\n",
      "loading bert.encoder.layer.8.output.LayerNorm.gamma from bert.encoder.layer.8.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.8.output.LayerNorm.beta from bert.encoder.layer.8.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.8.intermediate_query.dense.weight from bert.encoder.layer.8.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.8.intermediate_query.dense.bias from bert.encoder.layer.8.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.8.output_query.dense.weight from bert.encoder.layer.8.output_query.dense.weight\n",
      "loading bert.encoder.layer.8.output_query.dense.bias from bert.encoder.layer.8.output_query.dense.bias\n",
      "loading bert.encoder.layer.8.output_query.LayerNorm.gamma from bert.encoder.layer.8.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.8.output_query.LayerNorm.beta from bert.encoder.layer.8.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.9.attention.self.query.weight from bert.encoder.layer.9.attention.self.query.weight\n",
      "loading bert.encoder.layer.9.attention.self.query.bias from bert.encoder.layer.9.attention.self.query.bias\n",
      "loading bert.encoder.layer.9.attention.self.key.weight from bert.encoder.layer.9.attention.self.key.weight\n",
      "loading bert.encoder.layer.9.attention.self.key.bias from bert.encoder.layer.9.attention.self.key.bias\n",
      "loading bert.encoder.layer.9.attention.self.value.weight from bert.encoder.layer.9.attention.self.value.weight\n",
      "loading bert.encoder.layer.9.attention.self.value.bias from bert.encoder.layer.9.attention.self.value.bias\n",
      "loading bert.encoder.layer.9.attention.output.dense.weight from bert.encoder.layer.9.attention.output.dense.weight\n",
      "loading bert.encoder.layer.9.attention.output.dense.bias from bert.encoder.layer.9.attention.output.dense.bias\n",
      "loading bert.encoder.layer.9.attention.output.LayerNorm.gamma from bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.9.attention.output.LayerNorm.beta from bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.9.intermediate.dense.weight from bert.encoder.layer.9.intermediate.dense.weight\n",
      "loading bert.encoder.layer.9.intermediate.dense.bias from bert.encoder.layer.9.intermediate.dense.bias\n",
      "loading bert.encoder.layer.9.output.dense.weight from bert.encoder.layer.9.output.dense.weight\n",
      "loading bert.encoder.layer.9.output.dense.bias from bert.encoder.layer.9.output.dense.bias\n",
      "loading bert.encoder.layer.9.output.LayerNorm.gamma from bert.encoder.layer.9.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.9.output.LayerNorm.beta from bert.encoder.layer.9.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.9.intermediate_query.dense.weight from bert.encoder.layer.9.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.9.intermediate_query.dense.bias from bert.encoder.layer.9.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.9.output_query.dense.weight from bert.encoder.layer.9.output_query.dense.weight\n",
      "loading bert.encoder.layer.9.output_query.dense.bias from bert.encoder.layer.9.output_query.dense.bias\n",
      "loading bert.encoder.layer.9.output_query.LayerNorm.gamma from bert.encoder.layer.9.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.9.output_query.LayerNorm.beta from bert.encoder.layer.9.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.10.attention.self.query.weight from bert.encoder.layer.10.attention.self.query.weight\n",
      "loading bert.encoder.layer.10.attention.self.query.bias from bert.encoder.layer.10.attention.self.query.bias\n",
      "loading bert.encoder.layer.10.attention.self.key.weight from bert.encoder.layer.10.attention.self.key.weight\n",
      "loading bert.encoder.layer.10.attention.self.key.bias from bert.encoder.layer.10.attention.self.key.bias\n",
      "loading bert.encoder.layer.10.attention.self.value.weight from bert.encoder.layer.10.attention.self.value.weight\n",
      "loading bert.encoder.layer.10.attention.self.value.bias from bert.encoder.layer.10.attention.self.value.bias\n",
      "loading bert.encoder.layer.10.attention.output.dense.weight from bert.encoder.layer.10.attention.output.dense.weight\n",
      "loading bert.encoder.layer.10.attention.output.dense.bias from bert.encoder.layer.10.attention.output.dense.bias\n",
      "loading bert.encoder.layer.10.attention.output.LayerNorm.gamma from bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.10.attention.output.LayerNorm.beta from bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.10.crossattention.self.query.weight from bert.encoder.layer.10.crossattention.self.query.weight\n",
      "loading bert.encoder.layer.10.crossattention.self.query.bias from bert.encoder.layer.10.crossattention.self.query.bias\n",
      "loading bert.encoder.layer.10.crossattention.self.key.weight from bert.encoder.layer.10.crossattention.self.key.weight\n",
      "loading bert.encoder.layer.10.crossattention.self.key.bias from bert.encoder.layer.10.crossattention.self.key.bias\n",
      "loading bert.encoder.layer.10.crossattention.self.value.weight from bert.encoder.layer.10.crossattention.self.value.weight\n",
      "loading bert.encoder.layer.10.crossattention.self.value.bias from bert.encoder.layer.10.crossattention.self.value.bias\n",
      "loading bert.encoder.layer.10.crossattention.output.dense.weight from bert.encoder.layer.10.crossattention.output.dense.weight\n",
      "loading bert.encoder.layer.10.crossattention.output.dense.bias from bert.encoder.layer.10.crossattention.output.dense.bias\n",
      "loading bert.encoder.layer.10.crossattention.output.LayerNorm.gamma from bert.encoder.layer.10.crossattention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.10.crossattention.output.LayerNorm.beta from bert.encoder.layer.10.crossattention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.10.intermediate.dense.weight from bert.encoder.layer.10.intermediate.dense.weight\n",
      "loading bert.encoder.layer.10.intermediate.dense.bias from bert.encoder.layer.10.intermediate.dense.bias\n",
      "loading bert.encoder.layer.10.output.dense.weight from bert.encoder.layer.10.output.dense.weight\n",
      "loading bert.encoder.layer.10.output.dense.bias from bert.encoder.layer.10.output.dense.bias\n",
      "loading bert.encoder.layer.10.output.LayerNorm.gamma from bert.encoder.layer.10.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.10.output.LayerNorm.beta from bert.encoder.layer.10.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.10.intermediate_query.dense.weight from bert.encoder.layer.10.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.10.intermediate_query.dense.bias from bert.encoder.layer.10.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.10.output_query.dense.weight from bert.encoder.layer.10.output_query.dense.weight\n",
      "loading bert.encoder.layer.10.output_query.dense.bias from bert.encoder.layer.10.output_query.dense.bias\n",
      "loading bert.encoder.layer.10.output_query.LayerNorm.gamma from bert.encoder.layer.10.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.10.output_query.LayerNorm.beta from bert.encoder.layer.10.output_query.LayerNorm.bias\n",
      "loading bert.encoder.layer.11.attention.self.query.weight from bert.encoder.layer.11.attention.self.query.weight\n",
      "loading bert.encoder.layer.11.attention.self.query.bias from bert.encoder.layer.11.attention.self.query.bias\n",
      "loading bert.encoder.layer.11.attention.self.key.weight from bert.encoder.layer.11.attention.self.key.weight\n",
      "loading bert.encoder.layer.11.attention.self.key.bias from bert.encoder.layer.11.attention.self.key.bias\n",
      "loading bert.encoder.layer.11.attention.self.value.weight from bert.encoder.layer.11.attention.self.value.weight\n",
      "loading bert.encoder.layer.11.attention.self.value.bias from bert.encoder.layer.11.attention.self.value.bias\n",
      "loading bert.encoder.layer.11.attention.output.dense.weight from bert.encoder.layer.11.attention.output.dense.weight\n",
      "loading bert.encoder.layer.11.attention.output.dense.bias from bert.encoder.layer.11.attention.output.dense.bias\n",
      "loading bert.encoder.layer.11.attention.output.LayerNorm.gamma from bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.11.attention.output.LayerNorm.beta from bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.11.intermediate.dense.weight from bert.encoder.layer.11.intermediate.dense.weight\n",
      "loading bert.encoder.layer.11.intermediate.dense.bias from bert.encoder.layer.11.intermediate.dense.bias\n",
      "loading bert.encoder.layer.11.output.dense.weight from bert.encoder.layer.11.output.dense.weight\n",
      "loading bert.encoder.layer.11.output.dense.bias from bert.encoder.layer.11.output.dense.bias\n",
      "loading bert.encoder.layer.11.output.LayerNorm.gamma from bert.encoder.layer.11.output.LayerNorm.weight\n",
      "loading bert.encoder.layer.11.output.LayerNorm.beta from bert.encoder.layer.11.output.LayerNorm.bias\n",
      "loading bert.encoder.layer.11.intermediate_query.dense.weight from bert.encoder.layer.11.intermediate_query.dense.weight\n",
      "loading bert.encoder.layer.11.intermediate_query.dense.bias from bert.encoder.layer.11.intermediate_query.dense.bias\n",
      "loading bert.encoder.layer.11.output_query.dense.weight from bert.encoder.layer.11.output_query.dense.weight\n",
      "loading bert.encoder.layer.11.output_query.dense.bias from bert.encoder.layer.11.output_query.dense.bias\n",
      "loading bert.encoder.layer.11.output_query.LayerNorm.gamma from bert.encoder.layer.11.output_query.LayerNorm.weight\n",
      "loading bert.encoder.layer.11.output_query.LayerNorm.beta from bert.encoder.layer.11.output_query.LayerNorm.bias\n",
      "loading cls.predictions.bias from cls.predictions.bias\n",
      "loading cls.predictions.transform.dense.weight from cls.predictions.transform.dense.weight\n",
      "loading cls.predictions.transform.dense.bias from cls.predictions.transform.dense.bias\n",
      "loading cls.predictions.transform.LayerNorm.gamma from cls.predictions.transform.LayerNorm.weight\n",
      "loading cls.predictions.transform.LayerNorm.beta from cls.predictions.transform.LayerNorm.bias\n",
      "loading cls.predictions.decoder.weight from cls.predictions.decoder.weight\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "ckpt_path = \"/mnt/d/QFormer/ms_Q_former.ckpt\"\n",
    "param_convert(qFormer2.parameters_dict(), pt_params, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c84d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qFormer2.parameters_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb1adf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(16810:140443486904704,MainProcess):2023-06-13-10:39:10.238.71 [mindspore/train/serialization.py:163] The type of bert.embeddings.position_ids:Int64 in 'parameter_dict' is different from the type of it in 'net':Int32, then the type convert from Int64 to Int32 in the network.\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "ckpt_path = \"/mnt/d/QFormer/ms_Q_former.ckpt\"\n",
    "# param_convert(qFormer2.parameters_dict(), pt_params, ckpt_path)\n",
    "ms_param_dict = ms.load_checkpoint(ckpt_path)\n",
    "param_not_load = ms.load_param_into_net(qFormer2, ms_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6616df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_not_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dede8c3",
   "metadata": {},
   "source": [
    "#### forward compute validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e169d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inputs\n",
    "import torch\n",
    "from mindspore import Tensor\n",
    "query_tokens = torch.load(\"/mnt/d/QFormer/query_tokens.pth\", map_location=\"cpu\")\n",
    "image_embeds = torch.load(\"/mnt/d/QFormer/image_embeds.pth\", map_location=\"cpu\")\n",
    "image_atts = torch.load(\"/mnt/d/QFormer/image_atts.pth\", map_location=\"cpu\")\n",
    "\n",
    "\n",
    "query_tokens = Tensor.from_numpy(query_tokens.detach().numpy())\n",
    "image_embeds = Tensor.from_numpy(image_embeds.detach().numpy())\n",
    "image_atts = Tensor.from_numpy(image_atts.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24331d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_output2 = qFormer2.bert(\n",
    "           query_embeds=query_tokens,\n",
    "           encoder_hidden_states=image_embeds,\n",
    "           encoder_attention_mask=image_atts,\n",
    "           use_cache=True,\n",
    "           return_dict=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e47459de",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs2 = query_output2.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a41fd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[100, 32, 768], dtype=Float32, value=\n",
       "[[[ 6.36275828e-01, -1.23322546e+00, -2.64354914e-01 ...  3.03988844e-01, -1.24536790e-01,  1.63442934e+00],\n",
       "  [ 6.29782557e-01, -1.23557854e+00, -2.65074492e-01 ...  3.06480467e-01, -1.22025825e-01,  1.63047945e+00],\n",
       "  [ 6.36725545e-01, -1.23086798e+00, -2.63852268e-01 ...  3.02246600e-01, -1.23068646e-01,  1.63454282e+00],\n",
       "  ...\n",
       "  [ 6.39091134e-01, -1.22862041e+00, -2.61913985e-01 ...  2.99779266e-01, -1.24267742e-01,  1.63753891e+00],\n",
       "  [ 6.36037171e-01, -1.23141396e+00, -2.63945043e-01 ...  3.02295268e-01, -1.23219654e-01,  1.63482201e+00],\n",
       "  [ 6.40985966e-01, -1.22782278e+00, -2.62156427e-01 ...  2.97930896e-01, -1.24372512e-01,  1.63825858e+00]],\n",
       " [[ 7.07381606e-01, -1.16373205e+00, -2.78238177e-01 ...  2.48608321e-01, -2.10301340e-01,  1.58638465e+00],\n",
       "  [ 7.00246751e-01, -1.16481912e+00, -2.79877335e-01 ...  2.50751168e-01, -2.08516896e-01,  1.58291590e+00],\n",
       "  [ 7.07308173e-01, -1.16301501e+00, -2.77706742e-01 ...  2.47928858e-01, -2.08928034e-01,  1.58588326e+00],\n",
       "  ...\n",
       "  [ 7.10616052e-01, -1.16236544e+00, -2.75395095e-01 ...  2.45939195e-01, -2.10156426e-01,  1.58887386e+00],\n",
       "  [ 7.06011117e-01, -1.16375661e+00, -2.78264284e-01 ...  2.48219281e-01, -2.09015518e-01,  1.58586025e+00],\n",
       "  [ 7.12537229e-01, -1.16177428e+00, -2.75439471e-01 ...  2.44394302e-01, -2.10299060e-01,  1.58968651e+00]],\n",
       " [[ 7.24252999e-01, -1.23567927e+00, -2.88246304e-01 ...  2.60817438e-01, -8.00254643e-02,  1.54853129e+00],\n",
       "  [ 7.18069851e-01, -1.23694003e+00, -2.88492590e-01 ...  2.62765944e-01, -7.88393840e-02,  1.54424632e+00],\n",
       "  [ 7.24140942e-01, -1.23466682e+00, -2.87938297e-01 ...  2.60522455e-01, -7.87339509e-02,  1.54802454e+00],\n",
       "  ...\n",
       "  [ 7.27610290e-01, -1.23300374e+00, -2.86109746e-01 ...  2.58675188e-01, -7.94994310e-02,  1.55223191e+00],\n",
       "  [ 7.23363042e-01, -1.23503113e+00, -2.88004994e-01 ...  2.60467261e-01, -7.87078664e-02,  1.54844737e+00],\n",
       "  [ 7.29518056e-01, -1.23251438e+00, -2.86516786e-01 ...  2.57386625e-01, -7.93579742e-02,  1.55320168e+00]],\n",
       " ...\n",
       " [[ 6.76357508e-01, -1.11200666e+00, -2.26006418e-01 ...  2.88245499e-01, -8.73658732e-02,  1.50653291e+00],\n",
       "  [ 6.71902597e-01, -1.11387682e+00, -2.26914033e-01 ...  2.90131986e-01, -8.55854973e-02,  1.50358438e+00],\n",
       "  [ 6.75853252e-01, -1.11114633e+00, -2.25490198e-01 ...  2.87607193e-01, -8.61893147e-02,  1.50611472e+00],\n",
       "  ...\n",
       "  [ 6.78546131e-01, -1.10933888e+00, -2.23564640e-01 ...  2.85020769e-01, -8.77284333e-02,  1.50907779e+00],\n",
       "  [ 6.75196707e-01, -1.11157966e+00, -2.25696236e-01 ...  2.87515253e-01, -8.61980245e-02,  1.50641263e+00],\n",
       "  [ 6.80633307e-01, -1.10846496e+00, -2.23685890e-01 ...  2.83306599e-01, -8.80432054e-02,  1.50986648e+00]],\n",
       " [[ 7.71594405e-01, -1.15877390e+00, -2.41507739e-01 ...  2.57439166e-01, -1.37215674e-01,  1.60224843e+00],\n",
       "  [ 7.62813568e-01, -1.16208398e+00, -2.43516132e-01 ...  2.60432810e-01, -1.35031939e-01,  1.59723067e+00],\n",
       "  [ 7.71142662e-01, -1.15811157e+00, -2.41081789e-01 ...  2.56975055e-01, -1.35866508e-01,  1.60178936e+00],\n",
       "  ...\n",
       "  [ 7.73891628e-01, -1.15648746e+00, -2.38706470e-01 ...  2.54969537e-01, -1.36723861e-01,  1.60497081e+00],\n",
       "  [ 7.70087898e-01, -1.15887153e+00, -2.41483182e-01 ...  2.57033348e-01, -1.35872498e-01,  1.60189116e+00],\n",
       "  [ 7.75612295e-01, -1.15571904e+00, -2.38776982e-01 ...  2.53518462e-01, -1.36759013e-01,  1.60582209e+00]],\n",
       " [[ 6.79722130e-01, -1.20994210e+00, -3.18634093e-01 ...  2.40585759e-01, -2.73600608e-01,  1.58970749e+00],\n",
       "  [ 6.74137652e-01, -1.21164143e+00, -3.20073247e-01 ...  2.42340624e-01, -2.71730095e-01,  1.58666754e+00],\n",
       "  [ 6.79255188e-01, -1.20951760e+00, -3.18245292e-01 ...  2.40072742e-01, -2.72474587e-01,  1.58921766e+00],\n",
       "  ...\n",
       "  [ 6.82091057e-01, -1.20849621e+00, -3.15986782e-01 ...  2.38316014e-01, -2.73956746e-01,  1.59243941e+00],\n",
       "  [ 6.78087831e-01, -1.21008837e+00, -3.18573624e-01 ...  2.40151078e-01, -2.72388488e-01,  1.58933198e+00],\n",
       "  [ 6.84009850e-01, -1.20774853e+00, -3.15935433e-01 ...  2.36779511e-01, -2.73987144e-01,  1.59336603e+00]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41c98299",
   "metadata": {},
   "outputs": [],
   "source": [
    "qFormer2_dict = qFormer2.parameters_dict()\n",
    "for param in qFormer2.get_parameters():\n",
    "    if \"_query\" in param.name:\n",
    "        key_orig = param.name.replace(\"_query\", \"\")\n",
    "        param.data.assign_value(qFormer2_dict[key_orig].data.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa826935",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qFormer2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_output3 \u001b[38;5;241m=\u001b[39m \u001b[43mqFormer2\u001b[49m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m      2\u001b[0m            query_embeds\u001b[38;5;241m=\u001b[39mquery_tokens,\n\u001b[1;32m      3\u001b[0m            encoder_hidden_states\u001b[38;5;241m=\u001b[39mimage_embeds,\n\u001b[1;32m      4\u001b[0m            encoder_attention_mask\u001b[38;5;241m=\u001b[39mimage_atts,\n\u001b[1;32m      5\u001b[0m            use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m            return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m            )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qFormer2' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_17015/3432003637.py\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31mquery_output3 = qFormer2.bert(\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m           \u001b[0mquery_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m           \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m           \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_atts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m           \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query_output3 = qFormer2.bert(\n",
    "           query_embeds=query_tokens,\n",
    "           encoder_hidden_states=image_embeds,\n",
    "           encoder_attention_mask=image_atts,\n",
    "           use_cache=True,\n",
    "           return_dict=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e1c71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs3 = query_output3.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cf8df46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[100, 32, 768], dtype=Float32, value=\n",
       "[[[-1.66873068e-01,  2.97427177e-01, -2.62021162e-02 ...  3.27335268e-01,  2.37790465e-01,  3.30682784e-01],\n",
       "  [-1.92414388e-01,  3.16561610e-01, -2.34991126e-02 ...  3.23027343e-01,  2.48318821e-01,  2.72178739e-01],\n",
       "  [-1.43714860e-01,  2.96599984e-01, -3.72814797e-02 ...  3.19388598e-01,  2.52460420e-01,  3.09142053e-01],\n",
       "  ...\n",
       "  [-1.56098589e-01,  2.99248397e-01, -2.65768729e-02 ...  3.21254343e-01,  2.53752828e-01,  3.21896255e-01],\n",
       "  [-1.66216165e-01,  3.07389528e-01, -3.43378522e-02 ...  3.24267775e-01,  2.46809602e-01,  3.08967233e-01],\n",
       "  [-1.83831573e-01,  3.27633590e-01, -5.46987914e-02 ...  3.09363157e-01,  2.32047603e-01,  3.47589821e-01]],\n",
       " [[-1.41434312e-01,  4.00380135e-01, -1.92433968e-03 ...  4.00864512e-01,  1.98261693e-01,  3.16282243e-01],\n",
       "  [-1.73103839e-01,  4.23748493e-01, -1.23587251e-03 ...  3.97895575e-01,  2.10109860e-01,  2.52999455e-01],\n",
       "  [-1.12684198e-01,  4.02132392e-01, -1.48271471e-02 ...  3.98332119e-01,  2.14504108e-01,  2.88001180e-01],\n",
       "  ...\n",
       "  [-1.30963579e-01,  4.00052786e-01, -1.88611448e-04 ...  3.96831125e-01,  2.16784313e-01,  3.11588556e-01],\n",
       "  [-1.43376216e-01,  4.16432202e-01, -1.39992200e-02 ...  3.99478376e-01,  2.06545800e-01,  2.88823634e-01],\n",
       "  [-1.50953889e-01,  4.51500773e-01, -3.22719477e-02 ...  3.83779645e-01,  1.87884808e-01,  3.27758580e-01]],\n",
       " [[-1.47270024e-01,  3.81748319e-01,  1.44751146e-02 ...  3.23465288e-01,  1.39444381e-01,  2.99175203e-01],\n",
       "  [-1.75715417e-01,  4.02894318e-01,  1.60390101e-02 ...  3.20498884e-01,  1.50948286e-01,  2.40442231e-01],\n",
       "  [-1.23700581e-01,  3.84253919e-01,  5.23090363e-04 ...  3.17838341e-01,  1.53856605e-01,  2.73884952e-01],\n",
       "  ...\n",
       "  [-1.42265365e-01,  3.84476542e-01,  1.29200891e-02 ...  3.17441046e-01,  1.54930890e-01,  2.93106407e-01],\n",
       "  [-1.49857804e-01,  3.96834493e-01,  4.14592400e-03 ...  3.24179083e-01,  1.46741420e-01,  2.75952101e-01],\n",
       "  [-1.56922922e-01,  4.18857545e-01, -2.00597383e-02 ...  3.18064690e-01,  1.33324847e-01,  3.14523816e-01]],\n",
       " ...\n",
       " [[-9.38331708e-02,  3.58132899e-01,  7.75353462e-02 ...  4.33380187e-01,  1.97083846e-01,  2.08216205e-01],\n",
       "  [-1.21600784e-01,  3.78639549e-01,  7.70971775e-02 ...  4.29246902e-01,  2.07221180e-01,  1.49730727e-01],\n",
       "  [-6.95236251e-02,  3.59818101e-01,  6.36993796e-02 ...  4.27056491e-01,  2.10384578e-01,  1.82537749e-01],\n",
       "  ...\n",
       "  [-8.41342658e-02,  3.57755750e-01,  7.81323090e-02 ...  4.26478535e-01,  2.12228656e-01,  2.02821821e-01],\n",
       "  [-9.37531441e-02,  3.73520851e-01,  6.53411821e-02 ...  4.31857526e-01,  2.05528095e-01,  1.80205122e-01],\n",
       "  [-9.99550745e-02,  3.93344343e-01,  4.89144437e-02 ...  4.25096303e-01,  1.89286098e-01,  2.24388406e-01]],\n",
       " [[-7.07933009e-02,  4.13884014e-01, -6.42899275e-02 ...  3.98372710e-01,  2.69453108e-01,  2.92890847e-01],\n",
       "  [-9.98799801e-02,  4.33978915e-01, -5.94389774e-02 ...  3.94330114e-01,  2.79886127e-01,  2.33402118e-01],\n",
       "  [-4.37434614e-02,  4.14649457e-01, -7.61027336e-02 ...  3.92009705e-01,  2.85569310e-01,  2.71095663e-01],\n",
       "  ...\n",
       "  [-6.06522188e-02,  4.12782490e-01, -6.33387715e-02 ...  3.93636823e-01,  2.86476851e-01,  2.90284812e-01],\n",
       "  [-7.22723156e-02,  4.26281214e-01, -7.36476034e-02 ...  3.95618081e-01,  2.77514875e-01,  2.72093087e-01],\n",
       "  [-8.20252523e-02,  4.58309442e-01, -9.77832228e-02 ...  3.84698272e-01,  2.60244012e-01,  3.02753776e-01]],\n",
       " [[-1.57308161e-01,  3.37123245e-01, -7.56440014e-02 ...  3.17012340e-01,  1.73814654e-01,  3.24209452e-01],\n",
       "  [-1.86483830e-01,  3.60307902e-01, -7.36885220e-02 ...  3.14390332e-01,  1.87789530e-01,  2.59075612e-01],\n",
       "  [-1.30467594e-01,  3.39850187e-01, -8.80123079e-02 ...  3.12887907e-01,  1.90802380e-01,  2.95779586e-01],\n",
       "  ...\n",
       "  [-1.48812979e-01,  3.36349100e-01, -7.21683651e-02 ...  3.14492464e-01,  1.93761587e-01,  3.19994748e-01],\n",
       "  [-1.59181684e-01,  3.49121660e-01, -8.38259012e-02 ...  3.14645231e-01,  1.83272839e-01,  2.98870265e-01],\n",
       "  [-1.68670207e-01,  3.88786942e-01, -1.06522828e-01 ...  2.99590319e-01,  1.61127120e-01,  3.30201417e-01]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424cefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip2",
   "language": "python",
   "name": "blip2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
